{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a9bfc6",
   "metadata": {},
   "source": [
    "# Analysis of NSW Food Authority's Name & Shame Register\n",
    "\n",
    "The NSW Food Authority publishes lists of businesses that have breached or are alleged to have breached NSW food safety laws. Publishing the lists gives consumers more information to make decisions about where they eat or buy food. Individuals and businesses may receive either a penalty notice for their alleged offence or be prosecuted before a court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install html-table-parser-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab789c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import io\n",
    "from dotenv import load_dotenv #for loading env variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8bbe9",
   "metadata": {},
   "source": [
    "## 1. Get Existing Data from S3\n",
    "\n",
    "The Food Authority's Name & Shame website only displays the last 12 months of data. But since I started this repository (in June, 2024) I simply append any new data to the bottom of a dataset stored in aws S3. So step 1 of the overall process is to get this data\n",
    "\n",
    "#### Get access keys and read from aws S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "#get aws keys from .env file\n",
    "CLIENT_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "CLIENT_SECRET = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "\n",
    "#setup an s3 session for downloading and uploading file to s3\n",
    "session = boto3.Session(aws_access_key_id=CLIENT_ID,\n",
    "                        aws_secret_access_key=CLIENT_SECRET)\n",
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44652a4e",
   "metadata": {},
   "source": [
    "#### Declare aws parameters and read into a dataframe\n",
    "\n",
    "We will use `pre_df` (as in, *previous* dataframe) since this is the \"old\" data we are looking to update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the object and from what bucket the data will come from\n",
    "bucket_name='nsw-food-authority-name-and-shame'\n",
    "object_key='dataset.csv'\n",
    "version_id=\"\" #keep blank unless specific version is required\n",
    "\n",
    "if version_id =='':\n",
    "# Retrieve object from S3\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "else:\n",
    "    print(\"Downloading specific version: {}\".format(version_id))\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=object_key, VersionId = version_id)\n",
    "\n",
    "# Read the object's content into a Pandas DataFrame\n",
    "prev_df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "prev_df['notice_number'] = prev_df['notice_number'].astype(str) #convert to string for comparison\n",
    "print(\"   Dataset has been downloaded. Shape: {}\\n\".format(prev_df.shape))\n",
    "prev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cfa36",
   "metadata": {},
   "source": [
    "## 2. Get all notices that are currently on the Food Authority Website\n",
    "\n",
    "The function `scrape_tables` takes a url (which we've defined as the food authority Name and Shame Register) and iterates over child-page of the website. \n",
    "\n",
    "The result is `notice_df`; a dataframe of all the notices across all pages of the parent url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76809f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the parent page we are going to scrape\n",
    "url = \"https://www.foodauthority.nsw.gov.au/offences/penalty-notices\"\n",
    "\n",
    "print(\"iterate over the pages of url:\\n  {}\\n\".format(url))\n",
    "#scrape each of the pages and get the table of notices\n",
    "notice_df = utils.scrape_tables(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47e109",
   "metadata": {},
   "source": [
    "## 3. Compare the website to dataset\n",
    "\n",
    "We will now compare the notices found in Step 2, to the notices we already have in step 1\n",
    "\n",
    "an `old_notice_number` is one which, as of last scrape, had not yet been removed from the website. We know what hadn't been removed by filtering on only those which have a null `date_removed_from_website`. \n",
    "\n",
    "A `current_notice_number` is any notice live on the website now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f96732",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_notice_numbers = prev_df[prev_df['date_removed_from_website'].isnull()]['notice_number'].tolist()\n",
    "\n",
    "current_notice_numbers = notice_df['notice_number'].tolist()\n",
    "\n",
    "#get the difference of the above to determine new and removed notices.\n",
    "removed_notice_numbers = set(old_notice_numbers) - set(current_notice_numbers)\n",
    "new_notice_numbers = set(current_notice_numbers) - set(old_notice_numbers)\n",
    "\n",
    "print(\"{} notice_numbers removed\".format(len(removed_notice_numbers)))\n",
    "print(\"{} notice_numbers added\".format(len(new_notice_numbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf01fa",
   "metadata": {},
   "source": [
    "We now do a number of checks and run certain code based on these checks. \n",
    "\n",
    "E.g:\n",
    " * if **a notice was removed**\n",
    "     * update the `date_removed_from_website` field\n",
    " * if **a notice was added**\n",
    "     * open up a particular page to get the finer details\n",
    "     * then append to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if notice numbers were removed\n",
    "if len(removed_notice_numbers)==0:\n",
    "    print(\"   0 notice_numbers removed\")\n",
    "    \n",
    "else:\n",
    "    print(\"   {} notice_numbers removed\".format(len(removed_notice_numbers)))\n",
    "    prev_df = utils.handle_removed_notices(prev_df, removed_notice_numbers)\n",
    "\n",
    "#check if any new notice numbers\n",
    "if len(new_notice_numbers)==0:\n",
    "    print(\"   0 new notice_numbers added\")\n",
    "    result = prev_df #since no new entries, the result is just the old dataframe. \n",
    "    \n",
    "else:\n",
    "    print(\"   {} new notice_numbers found\".format(len(new_notice_numbers)))\n",
    "    print(new_notice_numbers)\n",
    "    \n",
    "    #we'll only work with these\n",
    "    notice_df = notice_df[notice_df['notice_number'].isin(new_notice_numbers)]\n",
    "    \n",
    "    #check they're unique\n",
    "    #check only unique numbers\n",
    "    if not len(notice_df['notice_number'].unique()) == len(notice_df):\n",
    "        raise ValueError(\"Not all policy numbers are unique\")\n",
    "        \n",
    "    else: #Get details per notice_number\n",
    "        print(\"4. Get penalty info...\")\n",
    "        #empty list to collect each row as a dictionary\n",
    "        penalties = []\n",
    "\n",
    "        for notice_number in new_notice_numbers:\n",
    "            print(\"   processing: {}\".format(notice_number))\n",
    "\n",
    "            # scrape the website\n",
    "            record = utils.get_penalty_notice(notice_number)    \n",
    "            penalties.append(record)\n",
    "            \n",
    "        print(\"Complete\\n\")\n",
    "\n",
    "        penalties_df = pd.DataFrame(penalties)\n",
    "        \n",
    "        utils.cleanup_dataframe(penalties_df)\n",
    "        notice_df = utils.join_dataframes(penalties_df, notice_df)\n",
    "        notice_df = utils.add_timestamp(notice_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75921c4e",
   "metadata": {},
   "source": [
    "## 4. Finalise the dataset and push back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean flag which triggers an upload or not\n",
    "need_to_upload=False\n",
    "\n",
    "if len(new_notice_numbers)>0:\n",
    "    need_to_upload=True\n",
    "    # add the new notices to the previous dataframe\n",
    "    result = pd.concat([prev_df, notice_df], ignore_index=True)\n",
    "\n",
    "if len(removed_notice_numbers)>0:\n",
    "    need_to_upload=True\n",
    "\n",
    "#if its ben identifed that the data needs to be uploaded\n",
    "if need_to_upload:\n",
    "    #overwrite dataset\n",
    "    print(\"5. Begin Upload back to S3...\")\n",
    "\n",
    "    # Reuse the bucket name but change object key\n",
    "    #object_key = 'test.csv'\n",
    "    print(\"This will write {} to bucket:{}\".format(object_key, bucket_name))\n",
    "    \n",
    "    result.sort_values(by='published_date', inplace=True)\n",
    "\n",
    "    # Convert DataFrame to CSV string\n",
    "    csv_buffer = io.StringIO()\n",
    "    result.to_csv(csv_buffer, index=False)  # Set index=False if you don't want row numbers\n",
    "    print(\"   Dataset Shape:{}\".format(result.shape))\n",
    "\n",
    "    # Upload to S3\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name, \n",
    "        Key=object_key, \n",
    "        Body=csv_buffer.getvalue()\n",
    "    )\n",
    "\n",
    "    print(\"object pushed to S3\")\n",
    "\n",
    "#There were no changes to data so no need to upload.\n",
    "else:\n",
    "    print(\"no changes to the website so dataset will not be updated at this time.\")\n",
    "\n",
    "result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
