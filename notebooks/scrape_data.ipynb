{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a9bfc6",
   "metadata": {},
   "source": [
    "# Analysis of NSW Food Authority's Name & Shame Register\n",
    "\n",
    "The NSW Food Authority publishes lists of businesses that have breached or are alleged to have breached NSW food safety laws. Publishing the lists gives consumers more information to make decisions about where they eat or buy food. Individuals and businesses may receive either a penalty notice for their alleged offence or be prosecuted before a court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ea03b-9f16-4566-86ea-96180636c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab789c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sys\n",
    "sys.path.append('../utils')  # For notebooks\n",
    "import utils \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import boto3\n",
    "import os\n",
    "import io\n",
    "from dotenv import load_dotenv #for loading env variables\n",
    "from github import Github #for pushing data to Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8bbe9",
   "metadata": {},
   "source": [
    "## 1. Get Existing Data from Github\n",
    "\n",
    "The Food Authority's Name & Shame website only displays the last 12 months of data. But since I started this repository (in June, 2024) I simply append any new data to the bottom of a dataset stored on Github. So step 1 of the overall process is to get this data\n",
    "\n",
    "#### Get access keys and read from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# GitHub Authentication (Replace placeholders with your information)\n",
    "access_token = os.environ.get(\"GITHUB_PERSONAL_ACCESS_TOKEN\")\n",
    "g = Github(access_token)\n",
    "\n",
    "# Repository and File Information\n",
    "repo_owner = \"liampearson\" \n",
    "repo_name = \"nsw-food-authority-name-and-shame\"\n",
    "file_path = \"data/dataset.csv\"\n",
    "\n",
    "# Get Repository\n",
    "repo = g.get_user(repo_owner).get_repo(repo_name)\n",
    "\n",
    "# Get File Contents\n",
    "try:\n",
    "    file_content = repo.get_contents(file_path)\n",
    "    \n",
    "    decoded_content = file_content.decoded_content.decode() # Decode if necessary    \n",
    "\n",
    "    prev_df = pd.read_csv(io.StringIO(decoded_content))\n",
    "\n",
    "    prev_df['notice_number'] = prev_df['notice_number'].astype(str) #convert to string for comparison\n",
    "    print(\"   Dataset has been downloaded. Shape: {}\\n\".format(prev_df.shape))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "\n",
    "prev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cfa36",
   "metadata": {},
   "source": [
    "## 2. Get all notices that are currently on the Food Authority Website\n",
    "\n",
    "The function `scrape_tables` takes a url (which we've defined as the food authority Name and Shame Register) and iterates over child-page of the website. \n",
    "\n",
    "The result is `notice_df`; a dataframe of all the notices across all pages of the parent url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76809f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the parent page we are going to scrape\n",
    "url = \"https://www.foodauthority.nsw.gov.au/offences/penalty-notices\"\n",
    "\n",
    "print(\"iterate over the pages of url:\\n  {}\\n\".format(url))\n",
    "#scrape each of the pages and get the table of notices\n",
    "notice_df = utils.scrape_tables(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47e109",
   "metadata": {},
   "source": [
    "## 3. Compare the website to dataset\n",
    "\n",
    "We will now compare the notices found in Step 2 (whats currently live on the website), to the notices we've scraped in the past. \n",
    "\n",
    "This will tell us:\n",
    "* what has been removed from the webstie (`removed_notice_numbers`)\n",
    "* what has been added to the website (`new_notice_numbers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f96732",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_notice_numbers = prev_df[prev_df['date_removed_from_website'].isnull()]['notice_number'].tolist()\n",
    "current_notice_numbers = notice_df['notice_number'].tolist()\n",
    "\n",
    "#get the difference of the above to determine new and removed notices.\n",
    "removed_notice_numbers = set(old_notice_numbers) - set(current_notice_numbers)\n",
    "new_notice_numbers = set(current_notice_numbers) - set(old_notice_numbers)\n",
    "\n",
    "print(\"{} notice_numbers removed\".format(len(removed_notice_numbers)))\n",
    "print(\"{} notice_numbers added\".format(len(new_notice_numbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf01fa",
   "metadata": {},
   "source": [
    "### Get More info per notice\n",
    "\n",
    "Each notice has a dedicated child-page that outlines extra details such as penalty_amount, circumstances, address etc.\n",
    "\n",
    "if **a notice was added**:\n",
    "* open up a particular page to get the finer details\n",
    "* then append to the dataset\n",
    "* when complete, join this extra information to the dataset\n",
    "\n",
    "if **a notice was removed**\n",
    "* update the `date_removed_from_website` field in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if notice numbers were removed\n",
    "if len(removed_notice_numbers)==0:\n",
    "    print(\"   0 notice_numbers removed\")\n",
    "    \n",
    "else:\n",
    "    print(\"   {} notice_numbers removed\".format(len(removed_notice_numbers)))\n",
    "    prev_df = utils.handle_removed_notices(prev_df, removed_notice_numbers)\n",
    "\n",
    "#check if any new notice numbers\n",
    "if len(new_notice_numbers)==0:\n",
    "    print(\"   0 new notice_numbers added\")\n",
    "    result = prev_df #since no new entries, the result is just the old dataframe. \n",
    "    \n",
    "else:\n",
    "    print(\"   {} new notice_numbers found\".format(len(new_notice_numbers)))\n",
    "    print(new_notice_numbers)\n",
    "    \n",
    "    #we'll only work with these\n",
    "    notice_df = notice_df[notice_df['notice_number'].isin(new_notice_numbers)]\n",
    "    \n",
    "    #check they're unique\n",
    "    #check only unique numbers\n",
    "    if not len(notice_df['notice_number'].unique()) == len(notice_df):\n",
    "        raise ValueError(\"Not all policy numbers are unique\")\n",
    "        \n",
    "    else: #Get details per notice_number\n",
    "        print(\"4. Get penalty info...\")\n",
    "        #empty list to collect each row as a dictionary\n",
    "        penalties = []\n",
    "\n",
    "        for notice_number in new_notice_numbers:\n",
    "            print(\"   processing: {}\".format(notice_number))\n",
    "\n",
    "            # scrape the website\n",
    "            record = utils.get_penalty_notice(notice_number)    \n",
    "            penalties.append(record)\n",
    "            \n",
    "        print(\"Complete\\n\")\n",
    "\n",
    "        penalties_df = pd.DataFrame(penalties)\n",
    "        \n",
    "        utils.cleanup_dataframe(penalties_df)\n",
    "        notice_df = utils.join_dataframes(penalties_df, notice_df)\n",
    "        notice_df = utils.add_timestamp(notice_df)\n",
    "        notice_df['council'] = notice_df['council'].str.upper()\n",
    "\n",
    "        result = pd.concat([prev_df, notice_df], ignore_index=True)\n",
    "\n",
    "    #regardless of if there were some removed added or nothing\n",
    "    result.drop('party_served_trade_name', axis=1, inplace=True)\n",
    "    \n",
    "    reorder_columns = ['published_date',\n",
    "                   'notice_number',\n",
    "                   'council',\n",
    "                   'trade_name',\n",
    "                   'suburb',\n",
    "                   'address',\n",
    "                   'postcode',\n",
    "                   'date_alleged_offence',\n",
    "                   'offence_code',\n",
    "                   'offence_description',\n",
    "                   'offence_circumstances',\n",
    "                   'party_served_company',\n",
    "                   'party_served_given_name',\n",
    "                   'party_served_surname',\n",
    "                   'penalty_amount',\n",
    "                   'penalty_issued_by',\n",
    "                   'penalty_date_served',\n",
    "                   'updated_date',\n",
    "                   'scrape_timestamp_utc',\n",
    "                   'date_removed_from_website']\n",
    "    \n",
    "    result = result[reorder_columns]\n",
    "    \n",
    "    result[\"published_date\"]= pd.to_datetime(result['published_date'], errors='coerce').dt.date\n",
    "    result['penalty_amount'] = result['penalty_amount'].astype(int)\n",
    "    result.sort_values(by=['published_date', 'council', 'suburb', 'trade_name'], inplace=True, ascending=[False, True, True,True])\n",
    "    \n",
    "    result.head()\n",
    "\n",
    "print(result[['trade_name', 'party_served_company']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75921c4e",
   "metadata": {},
   "source": [
    "## 4. Finalise the dataset and push back to Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_notice_numbers)>0 or len(removed_notice_numbers)>0:\n",
    "    \n",
    "    #overwrite dataset\n",
    "    print(\"5. Begin Upload to Github...\")\n",
    "    print(\"shape:\",result.shape)\n",
    "    repo = g.get_user(repo_owner).get_repo(repo_name)\n",
    "    \n",
    "    # Update the main dataset\n",
    "    file_content = result.to_csv(index=False)  # Convert to CSV\n",
    "\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        contents = repo.get_contents(file_path)\n",
    "        repo.update_file(contents.path, \"Updated dataset\", file_content, contents.sha)\n",
    "        print(f\"Updated existing file: {file_path}\")\n",
    "\n",
    "    except:  # File doesn't exist\n",
    "        repo.create_file(file_path, \"Added dataset\", file_content)\n",
    "        print(f\"Created new file: {file_path}\") \n",
    "\n",
    "#There were no changes to data so no need to upload.\n",
    "else:\n",
    "    print(\"no changes to the website so dataset will not be updated at this time.\")\n",
    "    print(\"shape:\",result.shape)\n",
    "    \n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352da248-1ff1-4fe0-b40e-f4f2ac708eed",
   "metadata": {},
   "source": [
    "## Create and push subset of latest notices to Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a51f2-83b1-40da-891e-cb14691497e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_results = result[result['published_date']==result['published_date'].max()][['published_date', 'council','trade_name', 'suburb','address', 'penalty_amount',\n",
    "       'offence_circumstances','offence_code']]\n",
    "\n",
    "print(\"shape:\",latest_results.shape)\n",
    "\n",
    "# Update the latest_result dataset\n",
    "file_content = latest_results.to_csv(index=False)  # Convert to CSV\n",
    "file_path = \"data/last_weeks_notices.csv\"\n",
    "\n",
    "try:\n",
    "    # Check if file exists\n",
    "    contents = repo.get_contents(file_path)\n",
    "    repo.update_file(contents.path, \"Updated dataset\", file_content, contents.sha)\n",
    "    print(f\"Updated existing file: {file_path}\")\n",
    "\n",
    "except:  # File doesn't exist\n",
    "    repo.create_file(file_path, \"Added dataset\", file_content)\n",
    "    print(f\"Created new file: {file_path}\")\n",
    "\n",
    "latest_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
